{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"PySpark","language":"","name":"pysparkkernel"},"language_info":{"codemirror_mode":{"name":"python","version":2},"mimetype":"text/x-python","name":"pyspark","pygments_lexer":"python2"},"colab":{"name":"Matrix Factorisation PySpark.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"Q6xxTY6dZ7hy","colab_type":"text"},"source":["# EDSA Movie Recommendation Challenge"]},{"cell_type":"markdown","metadata":{"id":"BjvhlJsNaCR_","colab_type":"text"},"source":["# Introduction"]},{"cell_type":"markdown","metadata":{"id":"rsmRTRZ-aPXV","colab_type":"text"},"source":["## Context\n","\n","In today’s technology driven world, recommender systems are socially and economically critical for ensuring that individuals can make appropriate choices surrounding the content they engage with on a daily basis. One application where this is especially true surrounds movie content recommendations; where intelligent algorithms can help viewers find great titles from tens of thousands of options."]},{"cell_type":"markdown","metadata":{"id":"mkkeXZgEaSP2","colab_type":"text"},"source":["## Problem Statement\n","With this context, EDSA is challenging you to construct a recommendation algorithm based on content or collaborative filtering, capable of accurately predicting how a user will rate a movie they have not yet viewed based on their historical preferences.\n","\n","Providing an accurate and robust solution to this challenge has immense economic potential, with users of the system being exposed to content they would like to view or purchase - generating revenue and platform affinity."]},{"cell_type":"markdown","metadata":{"id":"UqYVBDBCabDD","colab_type":"text"},"source":["## Evaluation\n","The evaluation metric for this competition is Root Mean Square Error. Root Mean Square Error (RMSE) is commonly used in regression analysis and forecasting, and measures the standard deviation of the residuals arising between predicted and actual observed values for a modelling process."]},{"cell_type":"markdown","metadata":{"id":"cYX6uwAkacQ-","colab_type":"text"},"source":["##### Matrix Factorisation\n","Matrix factorization is a class of collaborative filtering algorithms used in recommender systems. Matrix factorization algorithms work by decomposing the user-item interaction matrix into the product of two rectangular matrices that represent users and items and their preferences in a lower dimensional latent space. The first one has a row for each user, while the second has a column for each item. Further, the rows or columns associated with a specific user or item in the matrices are referred to as latent factors. Note that tt is possible to tune the expressive power of the model by changing the number of latent factors. It has been demonstrated that a matrix factorization with one latent factor is equivalent to a most popular or top popular recommender. Increasing the number of latent factor will improve personalization, therefore recommendation quality, until the number of factors becomes too high, at which point the model starts to overfit and the recommendation quality will decrease. A common strategy to avoid overfitting is to add regularization terms to the objective function. A user's rating is estimated using the following function\n","$$r_{ui} = \\mu + b_{u} + b_{i} + q_{i}^Tp_{u}$$\n","where $\\mu$ is the user's average rating, $b_{u}$ is the user bias, $b_{i}$ is the movie bias and the expression $q_{i}^Tp_{u}$ signifies the latent factors. The model's parameters are computed by minimising the following loss function using a gradient descent approach.\n","$$\\sum_{i=1}^N (r_{ui} - r_{ui})^2 + \\lambda(b_{i}^2 + b_{u}^2 + ||q_{i}||^2 + ||p_{u}||^2)$$\n","where $\\sum_{i=1}^N (r_{ui} - r_{ui})^2$ is the simple sum squared residuals, $\\lambda$ is a regularisation term that applies to the user bias, the movie bias as well as the latent factors. Note that regularisation can be different for each of the terms it applies to. In this section we implement models that utilise this approach to predict rating i.e matrix factorisation that uses alternating least squares (ALS) and one that decomposes the user-item matrix using Singular Values Decomposion (SVD). Further, note that these models have much more feasible implementation because they don't attempt to create the BIG matrix but rather use the matrices of latent factors to produce an estimate of the BIG matrix through an iterative approach."]},{"cell_type":"markdown","metadata":{"id":"eGzx43f1avlu","colab_type":"text"},"source":["###### Alternating Least Squares\n","Alternating Least Square (ALS) is a matrix factorization algorithm that runs itself in a parallel fashion. ALS is implemented in Apache Spark ML and built for a larges-scale collaborative filtering problems. While Apache Spark is written and developed using Scala there are various APIs that allow python, R and other programming languages to benefit from the technology. One of the major differences between ALS and the the Singular Value Decompsition approach is that regularisation in ALS is implemented as the L2 norm while SVD uses the L1 norm. Also, its training routine is different: ALS minimizes two loss functions alternatively; It first holds user matrix fixed and runs gradient descent with item matrix; then it holds item matrix fixed and runs gradient descent with user matrix and runs its gradient descent in parallel across multiple partitions of the underlying training data from a cluster of machines. Note also that the data can be residually distributed resulting in fast and highly scalable applications. \n","Here we use PySpark, python's API to Apache Spark. The logistics of setting up a cluster of computers is well beyond our expertise and as such, we utilise an Amazon Web Services managed service (Elastic MapReduce (EMR)). The service allows users to setup and run heavy jobs in a matter of minites by providing EC2 instance already installed with PySpark and all other dependencies such as Java, Hadoop etc. \n","This environment is being run on a cluster of 5 EC2 instances (including the Master node), each with the following specifications:\n","- Instance name - m5.2xlarge\n","- Number of CPU cores - 8\n","- RAM - 64Gib\n","- Instance storage - EBS only\n","- Pricing - spot\n","Lets first check to see if our environment is functioning as expected."]},{"cell_type":"code","metadata":{"id":"JDpDObdeWpKb","colab_type":"code","colab":{"referenced_widgets":["20a0133e4b4f4c2ea384b6a637c5cb4c",""]},"outputId":"05d28781-b88c-4686-fa6a-d9fccf940350"},"source":["spark"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"20a0133e4b4f4c2ea384b6a637c5cb4c","version_major":2,"version_minor":0},"text/plain":["VBox()"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Starting Spark application\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["<table>\n","<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>3</td><td>application_1595175669035_0004</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-9-139.eu-west-1.compute.internal:20888/proxy/application_1595175669035_0004/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-1-70.eu-west-1.compute.internal:8042/node/containerlogs/container_1595175669035_0004_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["SparkSession available as 'spark'.\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["<pyspark.sql.session.SparkSession object at 0x7f388fb38c50>"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wszMvYoseHX1","colab_type":"text"},"source":["Looks like it its. Next we load in the data from an S3 bucket that has been made public as there are no security issues with the dataset we are working with."]},{"cell_type":"code","metadata":{"id":"l19HCkGAWpKs","colab_type":"code","colab":{"referenced_widgets":["d9155ae8e89344dc951e030848f49796",""]},"outputId":"47328ef0-04b6-49ad-d831-7fb2edd2c589"},"source":["data = spark.read.csv(\"s3://predict/train.csv\", header=True)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d9155ae8e89344dc951e030848f49796","version_major":2,"version_minor":0},"text/plain":["VBox()"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"ww6L19lOWpKy","colab_type":"code","colab":{"referenced_widgets":["f8092cd9298f49faa7214a6bdca1aac6",""]},"outputId":"8313a3aa-2030-4b12-88cd-9818f517f201"},"source":["# show the data\n","data.show()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f8092cd9298f49faa7214a6bdca1aac6","version_major":2,"version_minor":0},"text/plain":["VBox()"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["+------+-------+------+----------+\n","|userId|movieId|rating| timestamp|\n","+------+-------+------+----------+\n","|  5163|  57669|   4.0|1518349992|\n","|106343|      5|   4.5|1206238739|\n","|146790|   5459|   5.0|1076215539|\n","|106362|  32296|   2.0|1423042565|\n","|  9041|    366|   3.0| 833375837|\n","|120949|  81768|   3.0|1289595242|\n","| 19630|  62049|   4.0|1246729817|\n","| 21066|   2282|   1.0| 945785907|\n","|117563| 120474|   4.0|1515108225|\n","|144018|   1997|   5.0|1109967647|\n","| 40858|   5025|   3.5|1090061607|\n","| 80119|  92259|   3.5|1435635534|\n","|  6063|  33493|   3.0|1236048966|\n","| 97844|   1784|   3.5|1111630438|\n","| 55909|   3978|   2.5|1111555006|\n","| 12942|  48394|   0.5|1280365881|\n","|161472|   3355|   4.0|1229975011|\n","|117890| 152077|   3.5|1488975758|\n","| 46581| 108190|   2.0|1465961865|\n","| 33970|   1265|   4.0|1463280090|\n","+------+-------+------+----------+\n","only showing top 20 rows"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LVg_iCoFeZXW","colab_type":"text"},"source":["We'd like to drop the timestamp column here. Matrix factorisation algorithms utiles only the ratings data."]},{"cell_type":"code","metadata":{"id":"6jr7kVmYWpK3","colab_type":"code","colab":{"referenced_widgets":["43a4802695aa4bf88b7e6beb2a990a59",""]},"outputId":"99d608c2-b109-421f-fe88-0ed2ccea5233"},"source":["columns_to_drop = ['timestamp']\n","data = data.drop(*columns_to_drop)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"43a4802695aa4bf88b7e6beb2a990a59","version_major":2,"version_minor":0},"text/plain":["VBox()"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"VC6pxEFqWpK-","colab_type":"code","colab":{"referenced_widgets":["f023a888ff594363a5ca8cfc94897ce0",""]},"outputId":"8461f4a5-5186-4308-de19-5c6b3bb777da"},"source":["# show a description of the data\n","data.describe().show()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f023a888ff594363a5ca8cfc94897ce0","version_major":2,"version_minor":0},"text/plain":["VBox()"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["+-------+------------------+------------------+------------------+\n","|summary|            userId|           movieId|            rating|\n","+-------+------------------+------------------+------------------+\n","|  count|          10000038|          10000038|          10000038|\n","|   mean| 81199.08814466505| 21389.11161287587|3.5333951730983424|\n","| stddev|46793.586155532874|39195.781053416096|1.0611240700473958|\n","|    min|                 1|                 1|               0.5|\n","|    max|             99999|             99999|               5.0|\n","+-------+------------------+------------------+------------------+"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kVnQurbbepnC","colab_type":"text"},"source":["Also, we recognise that the data was imported and parsed as strings. Below we convert $userId$, $movieId$ and $rating$ columns to integers and a double for ratings. A $double$ is a data structure identical to $numeric$\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"wmqPKadvWpLC","colab_type":"code","colab":{"referenced_widgets":["97becbe5b0fa4b84845b037cf08008ce",""]},"outputId":"ac5b5b0d-2bde-4bc8-b9ad-b9de608f29a3"},"source":["data = data.withColumn(\"userId\", data[\"userId\"].cast(\"integer\"))\n","data = data.withColumn(\"movieId\", data[\"movieId\"].cast(\"integer\"))\n","data = data.withColumn(\"rating\", data[\"rating\"].cast(\"double\"))"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"97becbe5b0fa4b84845b037cf08008ce","version_major":2,"version_minor":0},"text/plain":["VBox()"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"Dtp5rtgJfQEC","colab_type":"text"},"source":["Note that we'll also want to test the perfomance of the model before making a submission on kaggle. Here we use spark's method $.randomSplit$ and split the data table into 70% training and 30% tesing."]},{"cell_type":"code","metadata":{"id":"szprP7leWpLK","colab_type":"code","colab":{"referenced_widgets":["0b65f3199d734591872296b96955aedf",""]},"outputId":"de44ac3e-da82-47f5-9aef-ea486b95c99d"},"source":["(training, testing) = data.randomSplit([0.7, 0.3])"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0b65f3199d734591872296b96955aedf","version_major":2,"version_minor":0},"text/plain":["VBox()"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"PEDnYzmJfki3","colab_type":"text"},"source":["Now we want to import ALS. We also import the $RegressionEvaluater$ class that will allow us to evaluate the perfomance of the model on a test dataset using a metric of choice. We are challenged to minimise root mean squared error so thats what we'll use here."]},{"cell_type":"code","metadata":{"id":"i_ErrT0XWpLR","colab_type":"code","colab":{"referenced_widgets":["26c38d32cf1644478b7e62f9982c1dbf"]},"outputId":"b7852cd8-03ce-46d9-e3b7-1c2a23df7411"},"source":["from pyspark.ml.recommendation import ALS\n","from pyspark.ml.evaluation import RegressionEvaluator"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"26c38d32cf1644478b7e62f9982c1dbf","version_major":2,"version_minor":0},"text/plain":["VBox()"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["An error was encountered:\n","Session 3 did not reach idle status in time. Current status is busy.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"pmXjm-TAf7N8","colab_type":"text"},"source":["Next, we implement the model. Just like other machine learning algorithms, ALS has its own set of hyper-parameters. We would want to tune its hyper-parameters via hold-out validation or cross-validation. However, the compute can get quickly expensive on AWS. So we'll limit ourselves to arbitrarily chosen parameters. The set of parameters describe as follows:\n","- maxIter: the maximum number of iterations to run (defaults to 10)\n","- rank: the number of latent factors in the model (defaults to 10)\n","- regParam: the regularization parameter in ALS (L1) (defaults to 1.0)\n","We set maxIter to 20 here, rank to 10 and regParam to 0.05. Further, we'll need a strategy to deal with the cold start problem. ALS allows the user to either predict $np.nan$, a missing value or drop the users and movies affected by cold start altogether. Here we'll use the strategy of dropping them. See the cell below for the implementation."]},{"cell_type":"code","metadata":{"id":"guDeg_aaWpLX","colab_type":"code","colab":{"referenced_widgets":["3d30f365ff394dca8ee4646e49c528ba",""]},"outputId":"39642820-704e-49d9-d205-55b9f90cbc47"},"source":["als = ALS(\n","    userCol = \"userId\", \n","    itemCol = \"movieId\", \n","    ratingCol = \"rating\", \n","    coldStartStrategy = \"drop\",\n","    maxIter = 20, \n","    rank = 10,\n","    regParam = 0.05)\n","\n","model = als.fit(training)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3d30f365ff394dca8ee4646e49c528ba","version_major":2,"version_minor":0},"text/plain":["VBox()"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"IbWcYu-ghE0i","colab_type":"text"},"source":["Next, we want to test the model on the heldout test dataset. The rmse computed is 0.83."]},{"cell_type":"code","metadata":{"id":"ILyzBzasWpLc","colab_type":"code","colab":{"referenced_widgets":["34486d1d711d446a98366efdb4592158",""]},"outputId":"b132c6d8-0521-490d-e8a2-f7e32ce950ae"},"source":["## first get the prediction\n","predictions = model.transform(testing)\n","\n","## next, evaluate the model using the RegressionEvaluater class\n","evaluator = RegressionEvaluator(metricName = \"rmse\", labelCol = \"rating\", predictionCol = \"prediction\")\n","rmse = evaluator.evaluate(predictions)\n","\n","## and print the rmse\n","rmse"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"34486d1d711d446a98366efdb4592158","version_major":2,"version_minor":0},"text/plain":["VBox()"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["0.8310646804166654"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"V6gb95trhfwy","colab_type":"text"},"source":["Now we want to load the test data from the S3 bucket and transform it the same way we did the train data."]},{"cell_type":"code","metadata":{"id":"tJgCo1beWpLg","colab_type":"code","colab":{"referenced_widgets":["862a1eefd2b542938bf30b6cca842127",""]},"outputId":"e21d8814-3bdc-4b2f-f3d6-531e56eb5ec0"},"source":["test_data = spark.read.csv(\"s3://predict/test.csv\", header=True)\n","test_data = test_data.withColumn(\"userId\", test_data[\"userId\"].cast(\"integer\"))\n","test_data = test_data.withColumn(\"movieId\", test_data[\"movieId\"].cast(\"integer\"))"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"862a1eefd2b542938bf30b6cca842127","version_major":2,"version_minor":0},"text/plain":["VBox()"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"XnW_zGknWpLk","colab_type":"code","colab":{"referenced_widgets":["08926aaf43dd42d88e4be8b8f55e0d00",""]},"outputId":"bac328c6-b221-4b66-92d2-4ef960aff6ae"},"source":["test_data.show()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"08926aaf43dd42d88e4be8b8f55e0d00","version_major":2,"version_minor":0},"text/plain":["VBox()"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["+------+-------+\n","|userId|movieId|\n","+------+-------+\n","|     1|   2011|\n","|     1|   4144|\n","|     1|   5767|\n","|     1|   6711|\n","|     1|   7318|\n","|     1|   8405|\n","|     1|   8786|\n","|     2|    150|\n","|     2|    356|\n","|     2|    497|\n","|     2|    588|\n","|     2|    653|\n","|     2|   1080|\n","|     2|   1196|\n","|     2|   1198|\n","|     2|   1201|\n","|     2|   1299|\n","|     2|   1485|\n","|     2|   1580|\n","|     2|   1693|\n","+------+-------+\n","only showing top 20 rows"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jJC9095xhrsd","colab_type":"text"},"source":["Next, we make predictions on the test data."]},{"cell_type":"code","metadata":{"id":"ic2EBI_AWpLr","colab_type":"code","colab":{"referenced_widgets":["828357fd90264a2788fcf8de1e76aa7b",""]},"outputId":"d3097541-4bdb-4581-c2e1-1f640b365f8a"},"source":["# Evaluate the model by computing the RMSE on the test data\n","predictions = model.transform(test_data)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"828357fd90264a2788fcf8de1e76aa7b","version_major":2,"version_minor":0},"text/plain":["VBox()"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"-Ws0ShyoWpLx","colab_type":"code","colab":{"referenced_widgets":["7cd7af934d2340139c93ef228485ef52",""]},"outputId":"3d8c76cd-732f-4935-e5ba-9a13e6988462"},"source":["predictions.show()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7cd7af934d2340139c93ef228485ef52","version_major":2,"version_minor":0},"text/plain":["VBox()"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["+------+-------+------+----------+\n","|userId|movieId|rating|prediction|\n","+------+-------+------+----------+\n","|    12|    313|   3.0|     2.371|\n","|    12|    466|   3.0| 2.3660858|\n","|    12|    514|   3.0| 3.2177682|\n","|    12|    923|   5.0| 4.3434906|\n","|    12|   1343|   4.5| 3.3593106|\n","|    12|   1409|   1.0|  2.432028|\n","|    12|   1499|   1.0| 1.2541748|\n","|    12|   1895|   3.0| 2.8242474|\n","|    12|   2071|   4.5| 3.9559107|\n","|    12|   2902|   1.0| 1.8404977|\n","|    12|   3082|   3.0| 2.6318996|\n","|    12|   3210|   4.0| 3.4207344|\n","|    12|   3363|   4.0| 3.8230362|\n","|    12|   4979|   4.5| 3.9237773|\n","|    12|  51694|   4.0| 3.5921807|\n","|    12|    316|   3.0|  2.674575|\n","|    12|    368|   2.0|  3.022237|\n","|    12|    480|   4.0| 3.4556544|\n","|    12|    724|   2.0| 2.3973897|\n","|    12|   1196|   5.0| 4.2088485|\n","+------+-------+------+----------+\n","only showing top 20 rows"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7URoxiCHhzxk","colab_type":"text"},"source":["Import pandas to convert the spark dataframe to a pandas dataframe. Also install s3fs to be able to dump the predictions csv directly on the public S3 bucket."]},{"cell_type":"code","metadata":{"id":"Cnp3K97GWpL1","colab_type":"code","colab":{}},"source":["sc.install_pypi_package(\"pandas==0.25.1\")\n","sc.install_pypi_package(\"s3fs\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lc_jHQkvWpL7","colab_type":"code","colab":{"referenced_widgets":["e2be90290bba44808cb0f059700aa50c",""]},"outputId":"ea0b0e05-a9dd-4ad4-e3db-b96dff6a48e5"},"source":["submission = test_data.toPandas()\n","submission.head()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e2be90290bba44808cb0f059700aa50c","version_major":2,"version_minor":0},"text/plain":["VBox()"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["   userId  movieId  prediction\n","0      12        7    2.869163\n","1      12       21    3.558857\n","2      12       47    3.971080\n","3      12       50    4.427972\n","4      12      141    3.275634"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"oe030txMWpMB","colab_type":"code","colab":{"referenced_widgets":["cc2cb8676eb54c8280e36b8843b9e45e",""]},"outputId":"020e2b99-7213-4cc5-ee2d-d07a285a9e56"},"source":["# dump the submission file directly onto the S3 bucket.\n","submission.to_csv('s3://predict/submission_MF_Spark.csv', index = False)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cc2cb8676eb54c8280e36b8843b9e45e","version_major":2,"version_minor":0},"text/plain":["VBox()"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"pxoozz9aiN0y","colab_type":"text"},"source":["Finally, this model was submitted on the kaggle leaderboard and boosted our performance by the popularity recommender from 0.96 to 0.85. We acknowledge that this model could provide better results in we had done a search for hyper-parameters that provide the best scores in a cross validation setting, but given our limitations, these are satiffactory results. Now we go back to the main notebook and explore other alternative to ALS i.e SVD."]}]}